{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cfc8fa",
   "metadata": {},
   "source": [
    " # Web Scraper(Jobsite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c811c",
   "metadata": {},
   "source": [
    "## *This project is a web scraper that uses indeed jobsite as an example. It can be used to scrape any jobsite*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd99850",
   "metadata": {},
   "source": [
    "1. We start by importing the required libraries for dynamic scraping and presentation of the data. \n",
    "2. I used selenium along with BeautifulSoup for scraping and pandas library for presentation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb109154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a120ad",
   "metadata": {},
   "source": [
    "3. I have used the chrome webdriver but webdrivers of other browsers can also be used. \n",
    "4. To scrape multiple pages a for loop is used which loops through the changing url address(study the url address to see what changes when you go to the next page).\n",
    "5. I have set the loop to scrape the first three pages but i can change it to any number of pages.\n",
    "6. Then the url addresses are appended to list_url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "ser = Service(\"C:\\\\python codes\\\\chromedriver2\\\\chromedriver.exe\")\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)\n",
    "driver.maximize_window()\n",
    "\n",
    "list_url = [] \n",
    "job_titles = []\n",
    "companies= []\n",
    "locations = []\n",
    "date_posted = []\n",
    "scrapping_dates = []\n",
    "for page in range(0, 30, 10):\n",
    "\n",
    "    url = 'https://ca.indeed.com/jobs?q=data+scientist&l=Ontario&start=0' + str(page)\n",
    "    list_url.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c97d12",
   "metadata": {},
   "source": [
    "7. The next step is to loop through the list of web addresses and filter the html to get the relevant data. \n",
    "8. Beautiful Soup is used to thin the messy soup as it parses the html. Id and class tags are used to extract the required information such as job title, company name etc. \n",
    "9. Further things that can be extracted are the job description, link to apply, salary, rating etc. I am trying to keep it simple to follow. \n",
    "10. Try except blocks are used for error handling. \n",
    "11. Finally the extracted data gets appended to the respective lists which had been defined as empty lists in the beginning of the script. \n",
    "12. The time module is used to give the website a breather between each page scraped: there is a 3 secs wait time before the scraper moves to the next page. \n",
    "13. nce done the driver quits (closes all browser windows and ends driver session). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74300ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for webpage in list_url:\n",
    "    driver.get(webpage)\n",
    "\n",
    "\n",
    "    all_jobs = driver.find_elements(By.CLASS_NAME, 'slider_container')\n",
    "\n",
    "\n",
    "    for job in all_jobs:\n",
    "        result_html = job.get_attribute('innerHTML')\n",
    "        soup = BeautifulSoup(result_html, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            title = soup.find('h2', class_ = 'jobTitle').text.replace('\\n', '')\n",
    "\n",
    "        except:\n",
    "            title = \"None\"\n",
    "\n",
    "        try:\n",
    "            company = soup.find('span', class_='companyName').text\n",
    "\n",
    "        except:\n",
    "            company = \"None\"\n",
    "\n",
    "        try:\n",
    "            location = soup.find('div', class_='companyLocation').text\n",
    "        except:\n",
    "            location = \"None\"\n",
    "        try:\n",
    "            date = soup.find('span', class_= 'date').text.lstrip(\"Posted\")\n",
    "\n",
    "        except:\n",
    "            date = \"None\"\n",
    "\n",
    "        scrapping_dates.append(datetime.now())\n",
    "        job_titles.append(title)\n",
    "        companies.append(company)\n",
    "        locations.append(location)\n",
    "        date_posted.append(date)\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a8847",
   "metadata": {},
   "source": [
    "14. We're almost done! Now to present the data and see if anything actually happened after dancing around with the website.\n",
    "15. An empty dataframe is created using the pandas library and then each column is defined with a title and the list to be added to it. \n",
    "16. There are multiple other ways to create the data frame. Check out the pandas library official tutorials or w3school tutorials.\n",
    "17. The dataframe is then converted into a csv file so I can now also view the data in excel! Fun!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Job\"] = job_titles\n",
    "df[\"Company\"] = companies\n",
    "df[\"Location\"] = locations\n",
    "df[\"Date Posted\"] = date_posted\n",
    "df['Scrapping Date'] = scrapping_dates\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "print(df)\n",
    "df.to_csv(r'C:\\python codes\\file2.csv', header = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac0008",
   "metadata": {},
   "source": [
    "**Final thought: Always make sure you read the website's policy on webscrapping, using bots or spiders or any automated data extraction method. Web scrape ethically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7c185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
